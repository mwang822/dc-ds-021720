{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEATHOTSAUCE.COM SCRAPE\n",
    "\n",
    "https://heathotsauce.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all 19 pages to get names and urls\n",
    "\n",
    "names = []\n",
    "urls = []\n",
    "\n",
    "# go to each page (print progress)\n",
    "for i in range(1, 19):\n",
    "    url = 'https://heathotsauce.com/collections/all?page=' + str(i)\n",
    "    page = requests.get(url)\n",
    "    print (i, page)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    #in each page, scrape the names\n",
    "    for j in soup.findAll('span', class_='title product-name-custom'):\n",
    "        names.append(j.get_text())\n",
    "    for k in soup.findAll(itemprop = 'url'):\n",
    "        urls.append(k.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine names and urls into a df\n",
    "names_urls = pd.DataFrame({'names':names, 'urls':urls})\n",
    "\n",
    "# add url to start of urls\n",
    "names_urls['urls'] = names_urls['urls'].apply(lambda x: 'https://heathotsauce.com' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Go to each unique url, grab name, price, manufacturer, description\n",
    "\n",
    "## NOTE: I requested too many timesand was timed out. \n",
    "## Do this in batches, save as different df, then concat together\n",
    "\n",
    "name = []\n",
    "mfc = []\n",
    "price = []\n",
    "desc = []\n",
    "urls = []\n",
    "\n",
    "for n, url in enumerate(names_urls['urls'][:21]):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "#     name\n",
    "    try:\n",
    "        name.append(soup.find('h1', class_='product_name').get_text())\n",
    "    except:\n",
    "        name.append('')\n",
    "#     manufacturer\n",
    "    try:\n",
    "        mfc.append(soup.find('p', class_='vendor').get_text())\n",
    "    except:\n",
    "        mfc.append('')\n",
    "#     price\n",
    "    try:\n",
    "        price.append(soup.find('span', class_='money').get_text())\n",
    "    except:\n",
    "        price.append('')\n",
    "#     description\n",
    "    try:\n",
    "        desc.append(soup.find('div', class_='description').get_text())\n",
    "    except:\n",
    "        desc.append('')\n",
    "#     url\n",
    "    try:\n",
    "        urls.append(url)\n",
    "    except:\n",
    "        urls.append('')\n",
    "    \n",
    "#     print progress, if response != 200, stop kernel. you have been timed out\n",
    "    print (n, page)\n",
    "\n",
    "df_1 = pd.DataFrame({'names':name,\n",
    "                   'mfc':mfc,\n",
    "                   'price':price,\n",
    "                   'desc':desc,\n",
    "                   'url':urls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all df batches. strip manufacturers of \\n\n",
    "full_df = df_1\n",
    "full_df['mfc'] = full_df['mfc'].apply(lambda x: x.strip())\n",
    "full_df['desc'] = full_df['desc'].apply(lambda x: x.strip())\n",
    "\n",
    "# remove gift sets and empty rows\n",
    "full_df = full_df[full_df['mfc'] != 'Gift Set']\n",
    "full_df = full_df[full_df['desc'] != '']\n",
    "full_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# save df as csv\n",
    "full_df.to_csv('hot_sauces.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
